---  
layout: post   
title: Stock Research || 2.2. ì£¼ê°€ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
image: 04.thumbnail.png
tags:  
categories: stock-research
---

# 2. Data Preprocessing  
## 2.2. ì£¼ê°€ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
í˜„ì¬ ì‚¬ìš©í•˜ê³  ìˆëŠ” í†µí•© ì¢…ëª© ì£¼ê°€ ë°ì´í„°ëŠ” ì¢…ëª©ë§ˆë‹¤ ê°€ê²©ì´ ë‹¤ë¥´ë‹¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬í•˜ì—¬ ì„±ëŠ¥ì„ ì €í•˜ì‹œê¸°ëŠ” ì›ì¸ì´ ë  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì£¼ê°€ ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ íŒŒíŠ¸ì—ì„œ ìŠ¤ì¼€ì¼ë§ì€ ë§¤ìš° ì¤‘ìš”í•œ ìš”ì†Œë¼ê³ í•  ìˆ˜ ìˆë‹¤. `min-max scaling`ì„ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•˜ê³ , ì „ ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ëŠ” `div-close` ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì´ 4ê°€ì§€ ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥ì„ ë¹„êµí•´ë³¸ë‹¤. 


### ëª©ì°¨ 
- (1) ìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ   
- (2) ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ 

### í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ import 


```python
import pandas as pd
import numpy as np
from tqdm import tqdm
import FinanceDataReader as fdr
import pymysql

import warnings
warnings.filterwarnings('ignore')
    
import ta

from ipywidgets import interact, interact_manual
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.offline as pyo
%matplotlib inline
%pylab inline
pylab.rcParams['figure.figsize'] = (12,5)


# ìì£¼ì“°ëŠ” í•¨ìˆ˜ë“¤ì€ ëª¨ë“ˆí™” í•¨ 
import StockFunc as sf 
```

    Populating the interactive namespace from numpy and matplotlib


## âœ”ï¸ (1) ìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ 

### 1) min-max scaler (ì „ì²´ ì»¬ëŸ¼ + ì „ì²´ ë°ì´í„°)
ëª¨ë“  ì»¬ëŸ¼ì„ **ë…ë¦½ì **ìœ¼ë¡œ ì¢…ëª© ë³„ **ì „ì²´ë°ì´í„°**ì— ëŒ€í•´ `min-max scaling` í•œë‹¤. 


```python
def make_dataset_minmax(trading):
    from sklearn.preprocessing import MinMaxScaler
    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° 
    IF = open('../data/code_list.txt')
    lst_code = IF.readlines()

    lst_X = []
    lst_Y = []
    lst_code_date = []
    
    db_dsml = pymysql.connect(
        host = 'localhost', 
        port = 3306, 
        user = '[db username]', 
        passwd = '[db password]', 
        db = '[db name]', 
        charset = 'utf8'
    )
    cursor = db_dsml.cursor()
    
    
    for code in tqdm(lst_code): 
        code = code.strip()
        
        sql_query = '''
                    SELECT *
                    FROM stock_{0}
                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'
                    '''.format(code)

        
        stock = pd.read_sql(sql = sql_query, con = db_dsml) 
        stock['trading_value'] = stock['Close'] * stock['Volume']
        
        lst_stock = stock.values.tolist()

        
        # ğŸŒŸ scaling 
        df_temp = stock.drop(columns=['Date', 'Change', 'Next Change'])
        scaler = MinMaxScaler()
        scaled = scaler.fit_transform(df_temp)
        
        lst_stock_scaled = scaled.tolist()
         
        
        for idx, row in enumerate(lst_stock): 
            date, trading_value = row[0].date().strftime("%Y-%m-%d"), row[-1]
            if trading_value >= trading:
                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ 
                    continue 
                
                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° 
                sub_stock = lst_stock_scaled[idx-9:idx+1] 

                # 10ì¼ê°„ì˜ ë°ì´í„° 
                lst_result = []
                for row2 in sub_stock:
                    lst_result += row2

                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ 
                label = int(row[7] >= 0.02)
                
                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ 
                lst_X.append(lst_result)
                lst_Y.append(label)
                lst_code_date.append([code, date])
            
    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)
```


```python
df_dataset = make_dataset_minmax(trading=1000000000)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [03:37<00:00,  7.18it/s]



```python
trainX_1, trainY_1, testX_1, testY_1, lst_code_date, lst_code_date_test = sf.split(df_dataset)

print('train dataset: ', trainX_1.shape, trainY_1.shape)
print('test dataset: ', testX_1.shape, testY_1.shape)
```

    train dataset:  (659095, 550) (659095,)
    test dataset:  (239818, 550) (239818,)



```python
from xgboost import XGBClassifier
xgb_1 = XGBClassifier(
                   n_jobs=40,
                   scale_pos_weight=4,
                   learning_rate=0.01,
                   max_depth=3,
                   n_estimators=500,
                   ) 

xgb_1.fit(trainX_1, trainY_1)
```

    [15:05:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.





    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                  colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
                  gamma=0, gpu_id=-1, importance_type=None,
                  interaction_constraints='', learning_rate=0.01, max_delta_step=0,
                  max_depth=3, min_child_weight=1, missing=nan,
                  monotone_constraints='()', n_estimators=500, n_jobs=40,
                  num_parallel_tree=1, predictor='auto', random_state=0,
                  reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,
                  tree_method='exact', validate_parameters=1, verbosity=None)




```python
sf.plot_evauate(trainX_1, trainY_1, testX_1, testY_1, xgb_1)
```


    
![png](output_9_0.png)
    



    
![png](output_9_1.png)
    


### 2) min-max scaler (ì „ì²´ ì»¬ëŸ¼ +  window size) 
ëª¨ë“  ì»¬ëŸ¼ì„ **ë…ë¦½ì **ìœ¼ë¡œ ì¢…ëª© ë³„ **10ì¼ì¹˜**ì— ëŒ€í•´ `min-max scaling` í•œë‹¤. 


```python
def make_dataset_minmax_window(trading):
    from sklearn.preprocessing import MinMaxScaler
    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° 
    IF = open('../data/code_list.txt')
    lst_code = IF.readlines()

    lst_X = []
    lst_Y = []
    lst_code_date = []
    
    db_dsml = pymysql.connect(
        host = 'localhost', 
        port = 3306, 
        user = '[db username]', 
        passwd = '[db password]', 
        db = '[db name]', 
        charset = 'utf8'
    )
    cursor = db_dsml.cursor()
    
    
    for code in tqdm(lst_code): 
        code = code.strip()
        
        sql_query = '''
                    SELECT *
                    FROM stock_{0}
                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'
                    '''.format(code)

        
        stock = pd.read_sql(sql = sql_query, con = db_dsml) 
        stock['trading_value'] = stock['Close'] * stock['Volume']
        
        lst_stock = stock.values.tolist()

        
        for idx, row in enumerate(lst_stock): 
            date, trading_value = row[0].date().strftime("%Y-%m-%d"), row[-1]
            if trading_value >= trading:
                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ 
                    continue 
                
                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° 
                arr_sub_stock = np.array(lst_stock[idx-9:idx+1])

                # ğŸŒŸ scaling 
                arr_temp = np.concatenate((arr_sub_stock[:, 1:6], arr_sub_stock[:, 8:]), axis=1) 
                scaler = MinMaxScaler()
                scaled = scaler.fit_transform(np.array(arr_temp))

                lst_sub_stock_scaled = scaled.tolist()
                
                # 10ì¼ê°„ì˜ ë°ì´í„° 
                lst_result = []
                for row2 in lst_sub_stock_scaled:
                    lst_result += row2

                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ 
                label = int(row[7] >= 0.02)
                
                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ 
                lst_X.append(lst_result)
                lst_Y.append(label)
                lst_code_date.append([code, date])
            
    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)
```


```python
df_dataset = make_dataset_minmax_window(trading=1000000000)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [05:53<00:00,  4.41it/s]



```python
trainX_2, trainY_2, testX_2, testY_2, lst_code_date, lst_code_date_test = sf.split(df_dataset)

print('train dataset: ', trainX_2.shape, trainY_2.shape)
print('test dataset: ', testX_2.shape, testY_2.shape)
```

    train dataset:  (659095, 550) (659095,)
    test dataset:  (239818, 550) (239818,)



```python
from xgboost import XGBClassifier
xgb_2 = XGBClassifier(
                   n_jobs=40,
                   scale_pos_weight=4,
                   learning_rate=0.01,
                   max_depth=3,
                   n_estimators=500,
                   ) 

xgb_2.fit(trainX_2, trainY_2)
```

    [16:05:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.





    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                  colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
                  gamma=0, gpu_id=-1, importance_type=None,
                  interaction_constraints='', learning_rate=0.01, max_delta_step=0,
                  max_depth=3, min_child_weight=1, missing=nan,
                  monotone_constraints='()', n_estimators=500, n_jobs=40,
                  num_parallel_tree=1, predictor='auto', random_state=0,
                  reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,
                  tree_method='exact', validate_parameters=1, verbosity=None)




```python
sf.plot_evauate(trainX_2, trainY_2, testX_2, testY_2, xgb_2)
```


    
![png](output_15_0.png)
    



    
![png](output_15_1.png)
    


### 3) min-max scaler (ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ + ì „ì²´ ë°ì´í„°) 
- **ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼:** ìº”ë“¤ì°¨íŠ¸ì˜ ìµœì†Ÿê°’, ìµœëŒ“ê°’ìœ¼ë¡œ min-max scaling
- **ë‚˜ë¨¸ì§€ ì»¬ëŸ¼:** scaling í•˜ì§€ ì•ŠìŒ


```python
def make_dataset_minmax_price(trading):
    from sklearn.preprocessing import MinMaxScaler
    
    col_price = ['Open', 'High', 'Low', 'Close', 'MA5', 'MA20', 'MA60', 'MA120', 
               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',
               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   

    col_etc = ['Volume', 'MFI', 'ADI', 'OBV',
           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',
           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',
           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',
           'WR', 'AO', 'ROC', 'PPO', 'PVO', 'trading_value']
    
    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° 
    IF = open('../data/code_list.txt')
    lst_code = IF.readlines()

    lst_X = []
    lst_Y = []
    lst_code_date = []
    
    db_dsml = pymysql.connect(
        host = 'localhost', 
        port = 3306, 
        user = '[db username]', 
        passwd = '[db password]', 
        db = '[db name]', 
        charset = 'utf8'
    )
    cursor = db_dsml.cursor()
    
    
    for code in tqdm(lst_code): 
        code = code.strip()
        
        sql_query = '''
                    SELECT *
                    FROM stock_{0}
                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'
                    '''.format(code)

        
        stock = pd.read_sql(sql = sql_query, con = db_dsml) 
        stock['trading_value'] = stock['Close'] * stock['Volume']
        
        lst_stock = stock.values.tolist()

        
        # ğŸŒŸ scaling
        # 1) ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ 
        df_price = stock[col_price]
        minimum = df_price['Low'].min()
        maximum = df_price['High'].max()
        df_price_scaled = df_price.apply(lambda x: (x-minimum) / (maximum-minimum))
        
        # 2) ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ 
        df_etc = stock[col_etc]
        
        df_scaled = pd.concat([df_price_scaled, df_etc_scaled], axis=1)
        
        lst_stock_scaled = df_scaled.values.tolist()
         
            
        for idx, row in enumerate(lst_stock): 
            date, trading_value = row[0].date().strftime("%Y-%m-%d"), row[-1]
            if trading_value >= trading:
                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ 
                    continue 
                
                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° 
                sub_stock = lst_stock_scaled[idx-9:idx+1] 

                # 10ì¼ê°„ì˜ ë°ì´í„° 
                lst_result = []
                for row2 in sub_stock:
                    lst_result += row2

                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ 
                label = int(row[7] >= 0.02)
                
                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ 
                lst_X.append(lst_result)
                lst_Y.append(label)
                lst_code_date.append([code, date])
            
    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)
```


```python
df_dataset = make_dataset_minmax_price(trading=1000000000)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [03:53<00:00,  6.68it/s]



```python
trainX_3, trainY_3, testX_3, testY_3, lst_code_date, lst_code_date_test = sf.split(df_dataset)

print('train dataset: ', trainX_3.shape, trainY_3.shape)
print('test dataset: ', testX_3.shape, testY_3.shape)
```

    train dataset:  (659095, 550) (659095,)
    test dataset:  (239818, 550) (239818,)



```python
from xgboost import XGBClassifier
xgb_3 = XGBClassifier(
                   n_jobs=40,
                   scale_pos_weight=4,
                   learning_rate=0.01,
                   max_depth=3,
                   n_estimators=500,
                   ) 

xgb_3.fit(trainX_3, trainY_3)
```

    [16:15:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.





    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                  colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
                  gamma=0, gpu_id=-1, importance_type=None,
                  interaction_constraints='', learning_rate=0.01, max_delta_step=0,
                  max_depth=3, min_child_weight=1, missing=nan,
                  monotone_constraints='()', n_estimators=500, n_jobs=40,
                  num_parallel_tree=1, predictor='auto', random_state=0,
                  reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,
                  tree_method='exact', validate_parameters=1, verbosity=None)




```python
sf.plot_evauate(trainX_3, trainY_3, testX_3, testY_3, xgb_3)
```


    
![png](output_21_0.png)
    



    
![png](output_21_1.png)
    


### 4) div-previous_close (ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ + ì „ì²´ ë°ì´í„°)
- **ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼:** ì „ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ë§
- **ë‚˜ë¨¸ì§€ ì»¬ëŸ¼:** ìŠ¤ì¼€ì¼ë§ í•˜ì§€ ì•ŠëŠ”ë‹¤.


```python
def make_dataset_div_previous_close(trading):
    from sklearn.preprocessing import MinMaxScaler
    
    col_price = ['Open', 'High', 'Low', 'Close', 'MA5', 'MA20', 'MA60', 'MA120', 
               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',
               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   

    col_etc = ['Volume', 'MFI', 'ADI', 'OBV',
           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',
           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',
           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',
           'WR', 'AO', 'ROC', 'PPO', 'PVO', 'trading_value']
    
    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° 
    IF = open('../data/code_list.txt')
    lst_code = IF.readlines()

    lst_X = []
    lst_Y = []
    lst_code_date = []
    
    db_dsml = pymysql.connect(
        host = 'localhost', 
        port = 3306, 
        user = '[db username]', 
        passwd = '[db password]', 
        db = '[db name]', 
        charset = 'utf8'
    )
    cursor = db_dsml.cursor()
    
    
    for code in tqdm(lst_code): 
        code = code.strip()
        
        sql_query = '''
                    SELECT *
                    FROM stock_{0}
                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'
                    '''.format(code)

        
        stock = pd.read_sql(sql = sql_query, con = db_dsml) 
        stock['PrevClose'] = stock['Close'].shift(1) # ì „ ë‚  ì¢…ê°€ ì»¬ëŸ¼ ì¶”ê°€
        stock.dropna(inplace=True)
        stock = stock.reset_index(drop=True)
        stock['trading_value'] = stock['Close'] * stock['Volume']
        lst_stock = stock.values.tolist()

        
        # ğŸŒŸ scaling
        # 1) ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ 
        df_price = stock[col_price]
        df_price_scaled = df_price.apply(lambda x: x / stock['PrevClose'])
        
        # 2) ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ 
        df_etc = stock[col_etc]
        
        df_scaled = pd.concat([df_price_scaled, df_etc_scaled], axis=1)
        lst_stock_scaled = df_scaled.values.tolist()

        
        for idx, row in enumerate(lst_stock): 
            date, trading_value = row[0].date().strftime("%Y-%m-%d"), row[-1]
            if trading_value >= trading:
                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ 
                    continue 
                
                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° 
                sub_stock = lst_stock_scaled[idx-9:idx+1] 

                # 10ì¼ê°„ì˜ ë°ì´í„° 
                lst_result = []
                for row2 in sub_stock:               
                    lst_result += row2

                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ 
                label = int(row[7] >= 0.02)
                
                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ 
                lst_X.append(lst_result)
                lst_Y.append(label)
                lst_code_date.append([code, date])
            
    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)
```


```python
df_dataset = make_dataset_div_previous_close(trading=1000000000)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [03:53<00:00,  6.70it/s]



```python
trainX_4, trainY_4, testX_4, testY_4, lst_code_date, lst_code_date_test = sf.split(df_dataset)

print('train dataset: ', trainX_4.shape, trainY_4.shape)
print('test dataset: ', testX_4.shape, testY_4.shape)
```

    train dataset:  (658364, 550) (658364,)
    test dataset:  (239817, 550) (239817,)



```python
from xgboost import XGBClassifier
xgb_4 = XGBClassifier(
                   n_jobs=40,
                   scale_pos_weight=4,
                   learning_rate=0.01,
                   max_depth=3,
                   n_estimators=500,
                   ) 

xgb_4.fit(trainX_4, trainY_4)
```

    [16:26:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.





    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                  colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
                  gamma=0, gpu_id=-1, importance_type=None,
                  interaction_constraints='', learning_rate=0.01, max_delta_step=0,
                  max_depth=3, min_child_weight=1, missing=nan,
                  monotone_constraints='()', n_estimators=500, n_jobs=40,
                  num_parallel_tree=1, predictor='auto', random_state=0,
                  reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,
                  tree_method='exact', validate_parameters=1, verbosity=None)




```python
sf.plot_evauate(trainX_4, trainY_4, testX_4, testY_4, xgb_4)
```


    
![png](output_27_0.png)
    



    
![png](output_27_1.png)
    


## âœ”ï¸ (2) ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ

ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ì´ì „ë³´ë‹¤ í¬ê²Œ ì„±ëŠ¥ í–¥ìƒì´ ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì¢…ëª© ë³„ ê°€ê²©ì˜ ë²”ìœ„ë¥¼ ì¡°ì •í•´ ì£¼ëŠ” ì‘ì—…ì€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜ê¹Œì§€ í•´ë³´ê³ , ë„¤ ê°€ì§€ ë°©ë²• ì¤‘ ì§€ì†ì ìœ¼ë¡œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì„ íƒí•˜ë„ë¡ í•œë‹¤. 

### ìˆ˜ìµë¥  ë¹„êµ 


```python
sf.compute_earnings_rate(lst_code_date_test, xgb_1, testX_1, threshold=0.75)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:04<00:00, 42.12it/s]

    Final earning rate : 462.37913 %


    



```python
sf.compute_earnings_rate(lst_code_date_test, xgb_2, testX_2, threshold=0.75)
```

    0it [00:00, ?it/s]

    Final earning rate : 0.0 %


    



```python
sf.compute_earnings_rate(lst_code_date_test, xgb_3, testX_3, threshold=0.75)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 222/222 [00:05<00:00, 43.23it/s]

    Final earning rate : 124.50311 %


    



```python
sf.compute_earnings_rate(lst_code_date_test, xgb_4, testX_4, threshold=0.75)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 260/260 [00:06<00:00, 42.78it/s]

    Final earning rate : 1487.42557 %


    


ì˜ˆì¸¡í™•ë¥  ì„ê³„ê°’ì„ 0.75ë¡œ ì„¤ì •í•˜ì˜€ëŠ”ë°, `4) ì „ ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ê¸°` ë°©ë²•ì˜ ìŠ¤ì¼€ì¼ë§ì´ ê°€ì¥ ë§¤ë§¤ ê°œìˆ˜ê°€ ë§ì•˜ë‹¤. 1ë…„ ë™ì•ˆ 130ë²ˆì˜ ë§¤ìˆ˜ê°€ ì¼ì–´ë‚¬ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ì™€ ë™ì‹œì— ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ìˆ˜ìµë¥ ì´ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜¨ ì „ ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ëŠ” ìŠ¤ì¼€ì¼ë§ ë°©ë²•ì„ ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•œë‹¤. 
